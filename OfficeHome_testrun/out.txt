Args:
	algorithm: Proto
	batch_size: 0
	checkpoint_freq: 100
	data_dir: /data
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	model: resnet50
	model_dir: /models
	output_dir: OfficeHome_testrun
	proto_dir: None
	seed: 0
	steps: 10
	test_envs: [-1]
	trial_seed: 0
HParams:
	batch_size: 12
	bottleneck_size: 1024
	class_balanced: False
	data_augmentation: True
	data_parallel: True
	dataset: OfficeHome
	domains_per_iter: 4
	groupdro_eta: 0.01
	irm_lambda: 100.0
	irm_penalty_anneal_iters: 500
	lr: 0.0001
	mixup: 1.0
	mixup_alpha: 0.2
	mldg_beta: 1.0
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 256
	mmd_gamma: 0.1
	model: resnet50
	model_dir: /models
	nonlinear_classifier: False
	proto_domains_per_iter: 4
	proto_log_avg_step: 50
	proto_log_train_step: 20
	proto_lr: 1e-06
	proto_model: /data/dubeya/domain_generalization/outputs/pacs/
	proto_train_frac: 0.2
	proto_weight_decay: 1e-05
	resnet_dropout: 0.0
	train_prototype: True
	weight_decay: 0.0001
Traceback (most recent call last):
  File "src/embeddings.py", line 616, in <module>
    main(args)
  File "src/embeddings.py", line 459, in main
    ) = _setup_datasets(args, hparams)
  File "src/embeddings.py", line 80, in _setup_datasets
    dataset = vars(datasets)[args.dataset](args.data_dir, args.test_envs, hparams)
  File "/content/drive/MyDrive/Colab Notebooks/OSnsmt/os-nsmt/dalib/domainbed/datasets.py", line 138, in __init__
    super().__init__(self.dir, test_envs, True, hparams)
  File "/content/drive/MyDrive/Colab Notebooks/OSnsmt/os-nsmt/dalib/domainbed/datasets.py", line 72, in __init__
    environments = [f.name for f in os.scandir(root) if f.is_dir()]
FileNotFoundError: [Errno 2] No such file or directory: '/data/office_home/'
Args:
	algorithm: Proto
	batch_size: 0
	checkpoint_freq: 100
	data_dir: ../OfficeHomeDataset_10072016
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	model: resnet50
	model_dir: /models
	output_dir: OfficeHome_testrun
	proto_dir: None
	seed: 0
	steps: 10
	test_envs: [-1]
	trial_seed: 0
HParams:
	batch_size: 12
	bottleneck_size: 1024
	class_balanced: False
	data_augmentation: True
	data_parallel: True
	dataset: OfficeHome
	domains_per_iter: 4
	groupdro_eta: 0.01
	irm_lambda: 100.0
	irm_penalty_anneal_iters: 500
	lr: 0.0001
	mixup: 1.0
	mixup_alpha: 0.2
	mldg_beta: 1.0
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 256
	mmd_gamma: 0.1
	model: resnet50
	model_dir: /models
	nonlinear_classifier: False
	proto_domains_per_iter: 4
	proto_log_avg_step: 50
	proto_log_train_step: 20
	proto_lr: 1e-06
	proto_model: /data/dubeya/domain_generalization/outputs/pacs/
	proto_train_frac: 0.2
	proto_weight_decay: 1e-05
	resnet_dropout: 0.0
	train_prototype: True
	weight_decay: 0.0001
Traceback (most recent call last):
  File "src/embeddings.py", line 615, in <module>
    main(args)
  File "src/embeddings.py", line 458, in main
    ) = _setup_datasets(args, hparams)
  File "src/embeddings.py", line 79, in _setup_datasets
    dataset = vars(datasets)[args.dataset](args.data_dir, args.test_envs, hparams)
  File "/content/drive/MyDrive/Colab Notebooks/OSnsmt/os-nsmt/dalib/domainbed/datasets.py", line 138, in __init__
    super().__init__(self.dir, test_envs, True, hparams)
  File "/content/drive/MyDrive/Colab Notebooks/OSnsmt/os-nsmt/dalib/domainbed/datasets.py", line 72, in __init__
    environments = [f.name for f in os.scandir(root) if f.is_dir()]
FileNotFoundError: [Errno 2] No such file or directory: '../OfficeHomeDataset_10072016/office_home/'
Args:
	algorithm: Proto
	batch_size: 0
	checkpoint_freq: 100
	data_dir: ../OfficeHomeDataset_10072016
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	model: resnet50
	model_dir: /models
	output_dir: OfficeHome_testrun
	proto_dir: None
	seed: 0
	steps: 10
	test_envs: [-1]
	trial_seed: 0
HParams:
	batch_size: 12
	bottleneck_size: 1024
	class_balanced: False
	data_augmentation: True
	data_parallel: True
	dataset: OfficeHome
	domains_per_iter: 4
	groupdro_eta: 0.01
	irm_lambda: 100.0
	irm_penalty_anneal_iters: 500
	lr: 0.0001
	mixup: 1.0
	mixup_alpha: 0.2
	mldg_beta: 1.0
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 256
	mmd_gamma: 0.1
	model: resnet50
	model_dir: /models
	nonlinear_classifier: False
	proto_domains_per_iter: 4
	proto_log_avg_step: 50
	proto_log_train_step: 20
	proto_lr: 1e-06
	proto_model: /data/dubeya/domain_generalization/outputs/pacs/
	proto_train_frac: 0.2
	proto_weight_decay: 1e-05
	resnet_dropout: 0.0
	train_prototype: True
	weight_decay: 0.0001
/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
Traceback (most recent call last):
  File "src/embeddings.py", line 615, in <module>
    main(args)
  File "src/embeddings.py", line 461, in main
    args, algorithm_dict, device, dataset, hparams, trn_d, tst_d
  File "src/embeddings.py", line 184, in _setup_algorithm
    hparams,
  File "/content/drive/MyDrive/Colab Notebooks/OSnsmt/os-nsmt/dalib/domainbed/algorithms_proto.py", line 51, in __init__
    self.network = nn.Sequential(self.featurizer, self.classifier)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 1131, in __getattr__
    type(self).__name__, name))
AttributeError: 'Proto' object has no attribute 'featurizer'
Args:
	algorithm: Proto
	batch_size: 0
	checkpoint_freq: 100
	data_dir: ../OfficeHomeDataset_10072016
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	model: resnet50
	model_dir: /models
	output_dir: OfficeHome_testrun
	proto_dir: None
	seed: 0
	steps: 10
	test_envs: [-1]
	trial_seed: 0
HParams:
	batch_size: 12
	bottleneck_size: 1024
	class_balanced: False
	data_augmentation: True
	data_parallel: True
	dataset: OfficeHome
	domains_per_iter: 4
	groupdro_eta: 0.01
	irm_lambda: 100.0
	irm_penalty_anneal_iters: 500
	lr: 0.0001
	mixup: 1.0
	mixup_alpha: 0.2
	mldg_beta: 1.0
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 256
	mmd_gamma: 0.1
	model: resnet50
	model_dir: /models
	nonlinear_classifier: False
	proto_domains_per_iter: 4
	proto_log_avg_step: 50
	proto_log_train_step: 20
	proto_lr: 1e-06
	proto_model: /data/dubeya/domain_generalization/outputs/pacs/
	proto_train_frac: 0.2
	proto_weight_decay: 1e-05
	resnet_dropout: 0.0
	train_prototype: True
	weight_decay: 0.0001
/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
Traceback (most recent call last):
  File "src/embeddings.py", line 615, in <module>
    main(args)
  File "src/embeddings.py", line 461, in main
    args, algorithm_dict, device, dataset, hparams, trn_d, tst_d
  File "src/embeddings.py", line 184, in _setup_algorithm
    hparams,
  File "/content/drive/MyDrive/Colab Notebooks/OSnsmt/os-nsmt/dalib/domainbed/algorithms_proto.py", line 51, in __init__
    self.network = nn.Sequential(self.featurizer, self.classifier)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 1131, in __getattr__
    type(self).__name__, name))
AttributeError: 'Proto' object has no attribute 'featurizer'
Args:
	algorithm: Proto
	batch_size: 0
	checkpoint_freq: 100
	data_dir: ../OfficeHomeDataset_10072016
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	model: resnet50
	model_dir: /models
	output_dir: OfficeHome_testrun
	proto_dir: None
	seed: 0
	steps: 10
	test_envs: [-1]
	trial_seed: 0
HParams:
	batch_size: 12
	bottleneck_size: 1024
	class_balanced: False
	data_augmentation: True
	data_parallel: True
	dataset: OfficeHome
	domains_per_iter: 4
	groupdro_eta: 0.01
	irm_lambda: 100.0
	irm_penalty_anneal_iters: 500
	lr: 0.0001
	mixup: 1.0
	mixup_alpha: 0.2
	mldg_beta: 1.0
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 256
	mmd_gamma: 0.1
	model: resnet50
	model_dir: /models
	nonlinear_classifier: False
	proto_domains_per_iter: 4
	proto_log_avg_step: 50
	proto_log_train_step: 20
	proto_lr: 1e-06
	proto_model: /data/dubeya/domain_generalization/outputs/pacs/
	proto_train_frac: 0.2
	proto_weight_decay: 1e-05
	resnet_dropout: 0.0
	train_prototype: True
	weight_decay: 0.0001
/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
Traceback (most recent call last):
  File "src/embeddings.py", line 615, in <module>
    main(args)
  File "src/embeddings.py", line 461, in main
    args, algorithm_dict, device, dataset, hparams, trn_d, tst_d
  File "src/embeddings.py", line 184, in _setup_algorithm
    hparams,
  File "/content/drive/MyDrive/Colab Notebooks/OSnsmt/os-nsmt/dalib/domainbed/algorithms_proto.py", line 57, in __init__
    self.ft_output_size = self.featurizer.n_outputs
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 1131, in __getattr__
    type(self).__name__, name))
AttributeError: 'Proto' object has no attribute 'featurizer'
Args:
	algorithm: Proto
	batch_size: 0
	checkpoint_freq: 100
	data_dir: ../OfficeHomeDataset_10072016
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	model: resnet50
	model_dir: /models
	output_dir: OfficeHome_testrun
	proto_dir: None
	seed: 0
	steps: 10
	test_envs: [-1]
	trial_seed: 0
HParams:
	batch_size: 12
	bottleneck_size: 1024
	class_balanced: False
	data_augmentation: True
	data_parallel: True
	dataset: OfficeHome
	domains_per_iter: 4
	groupdro_eta: 0.01
	irm_lambda: 100.0
	irm_penalty_anneal_iters: 500
	lr: 0.0001
	mixup: 1.0
	mixup_alpha: 0.2
	mldg_beta: 1.0
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 256
	mmd_gamma: 0.1
	model: resnet50
	model_dir: /models
	nonlinear_classifier: False
	proto_domains_per_iter: 4
	proto_log_avg_step: 50
	proto_log_train_step: 20
	proto_lr: 1e-06
	proto_model: /data/dubeya/domain_generalization/outputs/pacs/
	proto_train_frac: 0.2
	proto_weight_decay: 1e-05
	resnet_dropout: 0.0
	train_prototype: True
	weight_decay: 0.0001
/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
Downloading: "https://download.pytorch.org/models/resnet50-0676ba61.pth" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth
  0%|          | 0.00/97.8M [00:00<?, ?B/s]  1%|          | 808k/97.8M [00:00<00:12, 7.86MB/s]  2%|1         | 1.54M/97.8M [00:01<01:32, 1.09MB/s]  2%|1         | 1.89M/97.8M [00:01<01:51, 900kB/s]   2%|2         | 2.11M/97.8M [00:02<01:40, 1.00MB/s]  4%|4         | 4.39M/97.8M [00:02<00:27, 3.53MB/s]  6%|5         | 5.49M/97.8M [00:02<00:22, 4.36MB/s]  7%|6         | 6.38M/97.8M [00:03<00:46, 2.06MB/s]  7%|7         | 7.02M/97.8M [00:03<00:45, 2.08MB/s]  8%|7         | 7.52M/97.8M [00:03<00:42, 2.24MB/s]  9%|9         | 8.90M/97.8M [00:03<00:26, 3.53MB/s] 10%|9         | 9.58M/97.8M [00:04<00:49, 1.88MB/s] 10%|#         | 10.1M/97.8M [00:05<00:48, 1.90MB/s] 11%|#         | 10.5M/97.8M [00:05<00:57, 1.58MB/s] 11%|#1        | 10.8M/97.8M [00:05<00:53, 1.71MB/s] 13%|#2        | 12.2M/97.8M [00:05<00:27, 3.22MB/s] 13%|#3        | 12.9M/97.8M [00:06<00:47, 1.88MB/s] 14%|#3        | 13.3M/97.8M [00:06<00:48, 1.82MB/s] 14%|#4        | 13.7M/97.8M [00:07<00:48, 1.82MB/s] 14%|#4        | 14.0M/97.8M [00:07<01:08, 1.28MB/s] 15%|#4        | 14.2M/97.8M [00:08<01:28, 984kB/s]  15%|#4        | 14.4M/97.8M [00:08<01:33, 932kB/s] 15%|#4        | 14.6M/97.8M [00:09<02:05, 697kB/s] 15%|#5        | 14.7M/97.8M [00:09<03:01, 480kB/s] 15%|#5        | 14.8M/97.8M [00:10<03:35, 403kB/s] 15%|#5        | 14.8M/97.8M [00:10<03:37, 400kB/s] 15%|#5        | 14.9M/97.8M [00:11<06:31, 222kB/s] 15%|#5        | 14.9M/97.8M [00:11<06:59, 207kB/s] 15%|#5        | 15.0M/97.8M [00:11<07:26, 195kB/s] 17%|#6        | 16.2M/97.8M [00:12<01:04, 1.32MB/s] 18%|#8        | 18.0M/97.8M [00:12<00:25, 3.25MB/s] 19%|#9        | 18.8M/97.8M [00:13<00:44, 1.86MB/s] 20%|#9        | 19.3M/97.8M [00:13<00:45, 1.82MB/s] 20%|##        | 19.8M/97.8M [00:13<00:56, 1.45MB/s] 21%|##        | 20.1M/97.8M [00:14<00:50, 1.62MB/s] 23%|##2       | 22.1M/97.8M [00:14<00:21, 3.68MB/s] 23%|##3       | 22.9M/97.8M [00:14<00:29, 2.70MB/s] 24%|##4       | 23.6M/97.8M [00:15<00:35, 2.22MB/s] 25%|##4       | 24.1M/97.8M [00:15<00:34, 2.21MB/s] 25%|##5       | 24.9M/97.8M [00:15<00:26, 2.92MB/s] 27%|##6       | 26.0M/97.8M [00:15<00:18, 4.12MB/s] 27%|##7       | 26.7M/97.8M [00:16<00:35, 2.09MB/s] 28%|##7       | 27.3M/97.8M [00:16<00:35, 2.08MB/s] 28%|##8       | 27.7M/97.8M [00:17<00:41, 1.78MB/s] 31%|###       | 29.9M/97.8M [00:17<00:17, 4.02MB/s] 33%|###2      | 31.8M/97.8M [00:17<00:11, 5.99MB/s] 34%|###3      | 33.0M/97.8M [00:18<00:17, 3.86MB/s] 35%|###4      | 33.9M/97.8M [00:18<00:21, 3.15MB/s] 35%|###5      | 34.6M/97.8M [00:18<00:20, 3.27MB/s] 36%|###5      | 35.2M/97.8M [00:18<00:18, 3.57MB/s] 37%|###7      | 36.6M/97.8M [00:18<00:12, 5.15MB/s] 38%|###8      | 37.4M/97.8M [00:19<00:14, 4.48MB/s] 39%|###8      | 38.1M/97.8M [00:19<00:24, 2.53MB/s] 39%|###9      | 38.6M/97.8M [00:20<00:27, 2.22MB/s] 40%|###9      | 39.0M/97.8M [00:20<00:39, 1.56MB/s] 40%|####      | 39.2M/97.8M [00:20<00:37, 1.63MB/s] 40%|####      | 39.5M/97.8M [00:21<00:34, 1.76MB/s] 41%|####      | 39.8M/97.8M [00:21<00:55, 1.09MB/s] 41%|####      | 40.0M/97.8M [00:22<01:10, 856kB/s]  41%|####1     | 40.1M/97.8M [00:22<01:10, 854kB/s] 41%|####1     | 40.2M/97.8M [00:22<01:06, 907kB/s] 42%|####2     | 41.5M/97.8M [00:22<00:23, 2.50MB/s] 43%|####2     | 41.9M/97.8M [00:23<00:31, 1.87MB/s] 43%|####3     | 42.2M/97.8M [00:23<00:45, 1.29MB/s] 43%|####3     | 42.5M/97.8M [00:23<00:43, 1.34MB/s] 44%|####3     | 42.7M/97.8M [00:23<00:39, 1.44MB/s] 48%|####7     | 46.5M/97.8M [00:23<00:07, 6.81MB/s] 49%|####8     | 47.5M/97.8M [00:24<00:13, 3.88MB/s] 49%|####9     | 48.3M/97.8M [00:25<00:17, 3.05MB/s] 50%|####9     | 48.8M/97.8M [00:25<00:18, 2.81MB/s] 50%|#####     | 49.3M/97.8M [00:25<00:16, 3.01MB/s] 52%|#####1    | 50.6M/97.8M [00:25<00:11, 4.35MB/s] 52%|#####2    | 51.3M/97.8M [00:26<00:20, 2.38MB/s] 53%|#####2    | 51.8M/97.8M [00:26<00:20, 2.37MB/s] 54%|#####3    | 52.7M/97.8M [00:26<00:14, 3.21MB/s] 56%|#####5    | 54.4M/97.8M [00:26<00:08, 5.18MB/s] 58%|#####7    | 56.2M/97.8M [00:26<00:05, 7.47MB/s] 59%|#####8    | 57.4M/97.8M [00:27<00:11, 3.54MB/s] 60%|#####9    | 58.3M/97.8M [00:28<00:13, 3.04MB/s] 60%|######    | 59.0M/97.8M [00:28<00:11, 3.45MB/s] 63%|######3   | 61.9M/97.8M [00:28<00:05, 6.72MB/s] 66%|######6   | 64.6M/97.8M [00:28<00:03, 9.54MB/s] 68%|######7   | 66.2M/97.8M [00:29<00:07, 4.51MB/s] 69%|######8   | 67.3M/97.8M [00:29<00:07, 4.49MB/s] 70%|#######   | 68.9M/97.8M [00:29<00:05, 5.68MB/s] 73%|#######2  | 71.2M/97.8M [00:29<00:03, 8.12MB/s] 74%|#######4  | 72.7M/97.8M [00:29<00:02, 9.22MB/s] 76%|#######5  | 74.2M/97.8M [00:30<00:05, 4.66MB/s] 77%|#######6  | 75.3M/97.8M [00:31<00:05, 4.09MB/s] 78%|#######8  | 76.7M/97.8M [00:31<00:04, 5.21MB/s] 81%|########1 | 79.2M/97.8M [00:31<00:02, 7.89MB/s] 83%|########2 | 80.7M/97.8M [00:31<00:02, 7.94MB/s] 84%|########3 | 82.0M/97.8M [00:32<00:03, 4.33MB/s] 85%|########4 | 82.9M/97.8M [00:32<00:03, 3.92MB/s] 86%|########5 | 83.6M/97.8M [00:32<00:03, 3.94MB/s] 87%|########6 | 84.6M/97.8M [00:32<00:02, 4.67MB/s] 87%|########7 | 85.3M/97.8M [00:33<00:02, 4.95MB/s] 88%|########7 | 86.0M/97.8M [00:33<00:03, 3.70MB/s] 89%|########8 | 86.6M/97.8M [00:33<00:04, 2.40MB/s] 89%|########8 | 87.0M/97.8M [00:34<00:04, 2.40MB/s] 89%|########9 | 87.3M/97.8M [00:34<00:04, 2.43MB/s] 91%|######### | 88.6M/97.8M [00:34<00:02, 4.06MB/s] 91%|#########1| 89.2M/97.8M [00:35<00:04, 2.09MB/s] 92%|#########1| 89.7M/97.8M [00:35<00:04, 1.81MB/s] 93%|#########2| 90.7M/97.8M [00:35<00:02, 2.66MB/s] 94%|#########4| 92.2M/97.8M [00:35<00:01, 4.35MB/s] 95%|#########5| 93.0M/97.8M [00:36<00:01, 2.72MB/s] 96%|#########5| 93.7M/97.8M [00:36<00:01, 2.59MB/s] 96%|#########6| 94.1M/97.8M [00:36<00:01, 2.56MB/s] 97%|#########6| 94.6M/97.8M [00:37<00:01, 2.70MB/s] 98%|#########7| 95.6M/97.8M [00:37<00:00, 3.89MB/s] 98%|#########8| 96.2M/97.8M [00:38<00:00, 1.81MB/s] 99%|#########8| 96.6M/97.8M [00:38<00:00, 1.58MB/s] 99%|#########9| 97.0M/97.8M [00:38<00:00, 1.83MB/s]100%|##########| 97.8M/97.8M [00:38<00:00, 2.57MB/s]100%|##########| 97.8M/97.8M [00:38<00:00, 2.65MB/s]
::: PROTOTYPE TRAINING :::
==========================
Training prototype for 200 steps...
/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
prototype step 0, loss 2.59, accuracy 0.24, time 3.86
prototype step 20, loss 2.59, accuracy 0.21, time 2.86
prototype step 40, loss 2.64, accuracy 0.32, time 2.93
prototype step 60, loss 3.30, accuracy 0.16, time 2.61
prototype step 80, loss 2.11, accuracy 0.20, time 2.59
prototype step 100, loss 2.51, accuracy 0.19, time 2.49
prototype step 120, loss 2.09, accuracy 0.22, time 2.48
prototype step 140, loss 2.23, accuracy 0.24, time 2.46
prototype step 160, loss 1.91, accuracy 0.31, time 2.52
prototype step 180, loss 2.57, accuracy 0.28, time 2.45
::: PROTOTYPE EXTRACTION FOR 42 STEPS :::
=========================================
Prototype (test) extraction, domain 1 of 8 done...
Prototype (test) extraction, domain 2 of 8 done...
Prototype (test) extraction, domain 3 of 8 done...
Prototype (test) extraction, domain 4 of 8 done...
Prototype (test) extraction, domain 5 of 8 done...
Prototype (test) extraction, domain 6 of 8 done...
Prototype (test) extraction, domain 7 of 8 done...
Prototype (test) extraction, domain 8 of 8 done...
::: MAIN TRAINING :::
=====================
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          step          step_time    
0.0447991761  0.0309278351  0.0229095074  0.0297823597  0.0335022523  0.0405862458  0.0390131956  0.0378874856  0.0000000000  4.2643814087  0             2.3050267696 
Args:
	algorithm: Proto
	batch_size: 0
	checkpoint_freq: 100
	data_dir: ../OfficeHomeDataset_10072016
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	model: resnet50
	model_dir: /models
	output_dir: OfficeHome_testrun
	proto_dir: None
	seed: 0
	steps: 10
	test_envs: [-1]
	trial_seed: 0
HParams:
	batch_size: 12
	bottleneck_size: 1024
	class_balanced: False
	data_augmentation: True
	data_parallel: True
	dataset: OfficeHome
	domains_per_iter: 4
	groupdro_eta: 0.01
	irm_lambda: 100.0
	irm_penalty_anneal_iters: 500
	lr: 0.0001
	mixup: 1.0
	mixup_alpha: 0.2
	mldg_beta: 1.0
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 256
	mmd_gamma: 0.1
	model: resnet50
	model_dir: /models
	nonlinear_classifier: False
	proto_domains_per_iter: 4
	proto_log_avg_step: 50
	proto_log_train_step: 20
	proto_lr: 1e-06
	proto_model: /data/IvLabs/domain_embeddings/outputs/pacs/
	proto_train_frac: 0.2
	proto_weight_decay: 1e-05
	resnet_dropout: 0.0
	train_prototype: True
	weight_decay: 0.0001
/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
Downloading: "https://download.pytorch.org/models/resnet50-0676ba61.pth" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth
  0%|          | 0.00/97.8M [00:00<?, ?B/s]  1%|          | 0.98M/97.8M [00:00<00:11, 8.98MB/s]  2%|1         | 1.84M/97.8M [00:00<00:16, 6.20MB/s]  3%|2         | 2.47M/97.8M [00:01<01:11, 1.40MB/s]  3%|2         | 2.84M/97.8M [00:01<01:19, 1.25MB/s]  3%|3         | 3.11M/97.8M [00:02<01:40, 988kB/s]   6%|5         | 5.50M/97.8M [00:02<00:30, 3.14MB/s]  8%|8         | 7.85M/97.8M [00:02<00:17, 5.27MB/s]  9%|9         | 8.98M/97.8M [00:03<00:40, 2.32MB/s] 10%|9         | 9.77M/97.8M [00:04<00:37, 2.43MB/s] 11%|#         | 10.4M/97.8M [00:04<00:34, 2.68MB/s] 13%|#3        | 12.8M/97.8M [00:04<00:18, 4.81MB/s] 14%|#4        | 13.9M/97.8M [00:04<00:16, 5.24MB/s] 15%|#5        | 14.8M/97.8M [00:06<00:41, 2.09MB/s] 16%|#5        | 15.5M/97.8M [00:06<00:47, 1.82MB/s] 16%|#6        | 16.1M/97.8M [00:06<00:44, 1.94MB/s] 18%|#8        | 17.7M/97.8M [00:06<00:26, 3.14MB/s] 20%|#9        | 19.4M/97.8M [00:07<00:18, 4.47MB/s] 21%|##        | 20.3M/97.8M [00:08<00:42, 1.91MB/s] 21%|##1       | 21.0M/97.8M [00:09<00:46, 1.74MB/s] 22%|##2       | 21.8M/97.8M [00:09<00:36, 2.17MB/s] 24%|##4       | 23.8M/97.8M [00:09<00:20, 3.80MB/s] 25%|##5       | 24.8M/97.8M [00:09<00:17, 4.47MB/s] 26%|##6       | 25.8M/97.8M [00:10<00:38, 1.97MB/s] 27%|##7       | 26.5M/97.8M [00:11<00:36, 2.03MB/s] 28%|##7       | 27.1M/97.8M [00:11<00:36, 2.06MB/s] 29%|##9       | 28.6M/97.8M [00:11<00:21, 3.32MB/s] 30%|###       | 29.4M/97.8M [00:12<00:41, 1.74MB/s] 31%|###       | 30.0M/97.8M [00:13<00:43, 1.62MB/s] 31%|###1      | 30.4M/97.8M [00:13<00:50, 1.40MB/s] 33%|###3      | 32.5M/97.8M [00:13<00:23, 2.86MB/s] 36%|###5      | 35.0M/97.8M [00:13<00:13, 4.72MB/s] 37%|###6      | 36.0M/97.8M [00:15<00:28, 2.25MB/s] 38%|###7      | 36.7M/97.8M [00:15<00:28, 2.24MB/s] 38%|###8      | 37.3M/97.8M [00:15<00:28, 2.23MB/s] 39%|###9      | 38.5M/97.8M [00:16<00:20, 3.00MB/s] 40%|####      | 39.1M/97.8M [00:17<00:36, 1.67MB/s] 40%|####      | 39.6M/97.8M [00:17<00:39, 1.53MB/s] 41%|####      | 40.0M/97.8M [00:18<00:48, 1.25MB/s] 41%|####1     | 40.2M/97.8M [00:18<00:45, 1.33MB/s] 44%|####3     | 42.8M/97.8M [00:18<00:16, 3.57MB/s] 45%|####5     | 44.2M/97.8M [00:18<00:11, 4.78MB/s] 46%|####6     | 45.2M/97.8M [00:18<00:11, 4.99MB/s] 47%|####7     | 46.1M/97.8M [00:19<00:23, 2.30MB/s] 48%|####7     | 46.7M/97.8M [00:20<00:31, 1.72MB/s] 48%|####8     | 47.2M/97.8M [00:21<00:37, 1.43MB/s] 49%|####9     | 48.3M/97.8M [00:21<00:24, 2.14MB/s] 52%|#####1    | 50.5M/97.8M [00:21<00:12, 3.97MB/s] 53%|#####2    | 51.7M/97.8M [00:21<00:09, 4.98MB/s] 54%|#####4    | 52.8M/97.8M [00:22<00:21, 2.17MB/s] 55%|#####4    | 53.6M/97.8M [00:23<00:20, 2.25MB/s] 56%|#####6    | 54.8M/97.8M [00:23<00:14, 3.01MB/s] 58%|#####7    | 56.6M/97.8M [00:23<00:09, 4.58MB/s] 59%|#####9    | 58.0M/97.8M [00:23<00:07, 5.84MB/s] 60%|######    | 59.2M/97.8M [00:24<00:17, 2.34MB/s] 61%|######1   | 60.0M/97.8M [00:25<00:18, 2.12MB/s] 62%|######2   | 60.6M/97.8M [00:25<00:17, 2.20MB/s] 63%|######3   | 61.8M/97.8M [00:25<00:12, 2.98MB/s] 64%|######3   | 62.5M/97.8M [00:25<00:11, 3.09MB/s] 64%|######4   | 63.1M/97.8M [00:27<00:22, 1.59MB/s] 65%|######4   | 63.5M/97.8M [00:27<00:22, 1.57MB/s] 65%|######5   | 63.8M/97.8M [00:27<00:22, 1.58MB/s] 67%|######6   | 65.1M/97.8M [00:27<00:12, 2.75MB/s] 67%|######7   | 65.7M/97.8M [00:28<00:24, 1.37MB/s] 68%|######7   | 66.2M/97.8M [00:29<00:28, 1.17MB/s] 68%|######8   | 66.9M/97.8M [00:29<00:20, 1.62MB/s] 69%|######9   | 67.7M/97.8M [00:29<00:14, 2.20MB/s] 70%|#######   | 68.6M/97.8M [00:29<00:10, 2.85MB/s] 71%|#######   | 69.1M/97.8M [00:30<00:21, 1.37MB/s] 71%|#######1  | 69.6M/97.8M [00:31<00:21, 1.35MB/s] 71%|#######1  | 69.9M/97.8M [00:31<00:20, 1.44MB/s] 73%|#######3  | 71.9M/97.8M [00:31<00:08, 3.36MB/s] 76%|#######5  | 73.9M/97.8M [00:31<00:04, 5.61MB/s] 77%|#######6  | 75.0M/97.8M [00:31<00:04, 5.22MB/s] 78%|#######7  | 76.0M/97.8M [00:32<00:08, 2.55MB/s] 78%|#######8  | 76.7M/97.8M [00:33<00:07, 2.82MB/s] 80%|########  | 78.4M/97.8M [00:33<00:04, 4.40MB/s] 81%|########1 | 79.7M/97.8M [00:33<00:03, 5.46MB/s] 83%|########2 | 80.7M/97.8M [00:34<00:06, 2.86MB/s] 83%|########3 | 81.4M/97.8M [00:34<00:06, 2.77MB/s] 84%|########3 | 82.0M/97.8M [00:34<00:05, 2.96MB/s] 85%|########5 | 83.4M/97.8M [00:34<00:03, 4.27MB/s] 87%|########7 | 85.2M/97.8M [00:34<00:02, 6.11MB/s] 88%|########8 | 86.2M/97.8M [00:35<00:04, 2.86MB/s] 89%|########8 | 86.9M/97.8M [00:36<00:04, 2.54MB/s] 90%|########9 | 87.8M/97.8M [00:36<00:03, 3.10MB/s] 92%|#########1| 89.9M/97.8M [00:36<00:01, 5.26MB/s] 94%|#########3| 91.6M/97.8M [00:36<00:00, 7.04MB/s] 95%|#########4| 92.9M/97.8M [00:37<00:01, 3.59MB/s] 96%|#########5| 93.8M/97.8M [00:37<00:01, 3.42MB/s] 97%|#########6| 94.5M/97.8M [00:37<00:00, 3.51MB/s] 97%|#########7| 95.1M/97.8M [00:38<00:00, 3.81MB/s] 98%|#########7| 95.7M/97.8M [00:38<00:00, 2.21MB/s] 98%|#########8| 96.2M/97.8M [00:38<00:00, 2.42MB/s] 99%|#########8| 96.6M/97.8M [00:39<00:00, 2.00MB/s]100%|##########| 97.8M/97.8M [00:39<00:00, 2.61MB/s]
::: PROTOTYPE TRAINING :::
==========================
Training prototype for 200 steps...
/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
prototype step 0, loss 2.59, accuracy 0.24, time 3.57
prototype step 20, loss 2.59, accuracy 0.21, time 2.51
prototype step 40, loss 2.64, accuracy 0.32, time 4.72
prototype step 60, loss 3.30, accuracy 0.16, time 2.39
prototype step 80, loss 2.11, accuracy 0.20, time 2.29
prototype step 100, loss 2.51, accuracy 0.19, time 2.30
prototype step 120, loss 2.09, accuracy 0.22, time 2.29
prototype step 140, loss 2.23, accuracy 0.24, time 2.34
prototype step 160, loss 1.91, accuracy 0.31, time 2.29
prototype step 180, loss 2.57, accuracy 0.28, time 2.28
::: PROTOTYPE EXTRACTION FOR 42 STEPS :::
=========================================
Prototype (test) extraction, domain 1 of 8 done...
Prototype (test) extraction, domain 2 of 8 done...
Prototype (test) extraction, domain 3 of 8 done...
Prototype (test) extraction, domain 4 of 8 done...
Prototype (test) extraction, domain 5 of 8 done...
Prototype (test) extraction, domain 6 of 8 done...
Prototype (test) extraction, domain 7 of 8 done...
Prototype (test) extraction, domain 8 of 8 done...
::: MAIN TRAINING :::
=====================
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          step          step_time    
0.0447991761  0.0309278351  0.0229095074  0.0297823597  0.0335022523  0.0405862458  0.0390131956  0.0378874856  0.0000000000  4.2643814087  0             2.7199208736 
