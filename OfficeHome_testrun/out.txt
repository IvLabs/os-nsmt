Args:
	algorithm: Proto
	batch_size: 0
	checkpoint_freq: 100
	data_dir: /data
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	model: resnet50
	model_dir: /models
	output_dir: OfficeHome_testrun
	proto_dir: None
	seed: 0
	steps: 10
	test_envs: [-1]
	trial_seed: 0
HParams:
	batch_size: 12
	bottleneck_size: 1024
	class_balanced: False
	data_augmentation: True
	data_parallel: True
	dataset: OfficeHome
	domains_per_iter: 4
	groupdro_eta: 0.01
	irm_lambda: 100.0
	irm_penalty_anneal_iters: 500
	lr: 0.0001
	mixup: 1.0
	mixup_alpha: 0.2
	mldg_beta: 1.0
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 256
	mmd_gamma: 0.1
	model: resnet50
	model_dir: /models
	nonlinear_classifier: False
	proto_domains_per_iter: 4
	proto_log_avg_step: 50
	proto_log_train_step: 20
	proto_lr: 1e-06
	proto_model: /data/dubeya/domain_generalization/outputs/pacs/
	proto_train_frac: 0.2
	proto_weight_decay: 1e-05
	resnet_dropout: 0.0
	train_prototype: True
	weight_decay: 0.0001
Traceback (most recent call last):
  File "src/embeddings.py", line 616, in <module>
    main(args)
  File "src/embeddings.py", line 459, in main
    ) = _setup_datasets(args, hparams)
  File "src/embeddings.py", line 80, in _setup_datasets
    dataset = vars(datasets)[args.dataset](args.data_dir, args.test_envs, hparams)
  File "/content/drive/MyDrive/Colab Notebooks/OSnsmt/os-nsmt/dalib/domainbed/datasets.py", line 138, in __init__
    super().__init__(self.dir, test_envs, True, hparams)
  File "/content/drive/MyDrive/Colab Notebooks/OSnsmt/os-nsmt/dalib/domainbed/datasets.py", line 72, in __init__
    environments = [f.name for f in os.scandir(root) if f.is_dir()]
FileNotFoundError: [Errno 2] No such file or directory: '/data/office_home/'
Args:
	algorithm: Proto
	batch_size: 0
	checkpoint_freq: 100
	data_dir: ../OfficeHomeDataset_10072016
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	model: resnet50
	model_dir: /models
	output_dir: OfficeHome_testrun
	proto_dir: None
	seed: 0
	steps: 10
	test_envs: [-1]
	trial_seed: 0
HParams:
	batch_size: 12
	bottleneck_size: 1024
	class_balanced: False
	data_augmentation: True
	data_parallel: True
	dataset: OfficeHome
	domains_per_iter: 4
	groupdro_eta: 0.01
	irm_lambda: 100.0
	irm_penalty_anneal_iters: 500
	lr: 0.0001
	mixup: 1.0
	mixup_alpha: 0.2
	mldg_beta: 1.0
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 256
	mmd_gamma: 0.1
	model: resnet50
	model_dir: /models
	nonlinear_classifier: False
	proto_domains_per_iter: 4
	proto_log_avg_step: 50
	proto_log_train_step: 20
	proto_lr: 1e-06
	proto_model: /data/dubeya/domain_generalization/outputs/pacs/
	proto_train_frac: 0.2
	proto_weight_decay: 1e-05
	resnet_dropout: 0.0
	train_prototype: True
	weight_decay: 0.0001
Traceback (most recent call last):
  File "src/embeddings.py", line 615, in <module>
    main(args)
  File "src/embeddings.py", line 458, in main
    ) = _setup_datasets(args, hparams)
  File "src/embeddings.py", line 79, in _setup_datasets
    dataset = vars(datasets)[args.dataset](args.data_dir, args.test_envs, hparams)
  File "/content/drive/MyDrive/Colab Notebooks/OSnsmt/os-nsmt/dalib/domainbed/datasets.py", line 138, in __init__
    super().__init__(self.dir, test_envs, True, hparams)
  File "/content/drive/MyDrive/Colab Notebooks/OSnsmt/os-nsmt/dalib/domainbed/datasets.py", line 72, in __init__
    environments = [f.name for f in os.scandir(root) if f.is_dir()]
FileNotFoundError: [Errno 2] No such file or directory: '../OfficeHomeDataset_10072016/office_home/'
Args:
	algorithm: Proto
	batch_size: 0
	checkpoint_freq: 100
	data_dir: ../OfficeHomeDataset_10072016
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	model: resnet50
	model_dir: /models
	output_dir: OfficeHome_testrun
	proto_dir: None
	seed: 0
	steps: 10
	test_envs: [-1]
	trial_seed: 0
HParams:
	batch_size: 12
	bottleneck_size: 1024
	class_balanced: False
	data_augmentation: True
	data_parallel: True
	dataset: OfficeHome
	domains_per_iter: 4
	groupdro_eta: 0.01
	irm_lambda: 100.0
	irm_penalty_anneal_iters: 500
	lr: 0.0001
	mixup: 1.0
	mixup_alpha: 0.2
	mldg_beta: 1.0
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 256
	mmd_gamma: 0.1
	model: resnet50
	model_dir: /models
	nonlinear_classifier: False
	proto_domains_per_iter: 4
	proto_log_avg_step: 50
	proto_log_train_step: 20
	proto_lr: 1e-06
	proto_model: /data/dubeya/domain_generalization/outputs/pacs/
	proto_train_frac: 0.2
	proto_weight_decay: 1e-05
	resnet_dropout: 0.0
	train_prototype: True
	weight_decay: 0.0001
/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
Traceback (most recent call last):
  File "src/embeddings.py", line 615, in <module>
    main(args)
  File "src/embeddings.py", line 461, in main
    args, algorithm_dict, device, dataset, hparams, trn_d, tst_d
  File "src/embeddings.py", line 184, in _setup_algorithm
    hparams,
  File "/content/drive/MyDrive/Colab Notebooks/OSnsmt/os-nsmt/dalib/domainbed/algorithms_proto.py", line 51, in __init__
    self.network = nn.Sequential(self.featurizer, self.classifier)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 1131, in __getattr__
    type(self).__name__, name))
AttributeError: 'Proto' object has no attribute 'featurizer'
